# -*- coding: utf-8 -*-
"""Basic Qwen Chatbot (Inference Only).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bMG9r1gro9L23H4n02tSk9OXOuLdX8L2


    https://colab.research.google.com/drive/1bMG9r1gro9L23H4n02tSk9OXOuLdX8L2?usp=sharing
"""

pip install -q transformers gradio accelerate

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import gradio as gr

model_name = "Qwen/qwen1.5-1.8B"

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_name,
                                             device_map="auto",
                                             torch_dtype=torch.float16,
                                             trust_remote_code=True)

def generate_response(message, history=[]):
    # Clean message
    message = message.strip()

    # ---üõ°Ô∏è Input Checks---
    if len(message) > 500:
        return "‚ö†Ô∏è Input too long. Please shorten your message.", history

    # Too many zeros or meaningless repetition
    if message.count("0") > 100:
        return "‚ö†Ô∏è Too many repeating characters. Please ask a valid question.", history

    if not any(c.isalpha() for c in message):
        return "‚ö†Ô∏è Input must contain letters. Please rephrase your question.", history

    # ---üìú Build Prompt from History---
    prompt = ""
    for turn in history:
        prompt += f"[User]: {turn[0]}\n[Bot]: {turn[1]}\n"
    prompt += f"[User]: {message}\n[Bot]:"

    # ---üîç Debug Print for Prompt---
    print("====== PROMPT SENT TO MODEL ======")
    print(prompt)
    print("==================================")

    # ---üß† Generate Response---
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        temperature=0.7,
        do_sample=True,
        top_p=0.95,
        eos_token_id=tokenizer.eos_token_id,
    )

    # ---üîì Decode Output---
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # ---‚úÇÔ∏è Extract only the latest Bot response---
    bot_response = decoded.split("[Bot]:")[-1].strip()

    # Update conversation history
    history.append((message, bot_response))
    return bot_response, history


with gr.Blocks() as demo:
  gr.Markdown("## Qwen 1.5 chatbot demo ")
  chatbot = gr.Chatbot()
  msg = gr.Textbox(label="your Message")
  clear = gr.Button("clear chat")

  state = gr.State([])

  def user_input(user_input_text, history): # Renamed parameter to avoid conflict
    response, history = chat(user_input_text, history) # Calling the chat function
    return history, "", history

  msg.submit(user_input, [msg, state], [chatbot, msg, state])
  clear.click(lambda: None, None, chatbot, queue=False)

if __name__ == "__main__":
  demo.launch()

